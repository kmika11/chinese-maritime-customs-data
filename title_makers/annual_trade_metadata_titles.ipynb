{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1d2847",
   "metadata": {},
   "source": [
    "# PART I: TITLE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf8b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_language_detection import LanguageDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f13e67bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set globals\n",
    "\n",
    "#list of all ports in vols\n",
    "port_names = [\n",
    "    \"Aigun\",\n",
    "    \"Harbin District\",\n",
    "    \"Hunchun & Lungchingtsun\",\n",
    "    \"Moukden\",\n",
    "    \"Antung\",\n",
    "    \"Dairen\",\n",
    "    \"Newchwang\",\n",
    "    \"Chinwangtao & Tientsin\",\n",
    "    \"Lungkow & Chefoo\",\n",
    "    \"Kiaochow\",\n",
    "    \"Chungking & Wanhsien\",\n",
    "    \"Ichang\",\n",
    "    \"Shasi\",\n",
    "    \"Changsha\",\n",
    "    \"Yochow\",\n",
    "    \"Hankow\",\n",
    "    \"Kiukiang\",\n",
    "    \"Wuhu\",\n",
    "    \"Nanking\",\n",
    "    \"Chinkiang\",\n",
    "    \"Shanghai\",\n",
    "    \"Soochow\",\n",
    "    \"Hangchow\",\n",
    "    \"Ningpo\",\n",
    "    \"Wenchow\",\n",
    "    \"Santuao\",\n",
    "    \"Foochow\",\n",
    "    \"Amoy\",\n",
    "    \"Swatow\",\n",
    "    \"Canton\",\n",
    "    \"Kowloon\",\n",
    "    \"Lappa\",\n",
    "    \"Kongmoon\",\n",
    "    \"Samshui\",\n",
    "    \"Wuchow\",\n",
    "    \"Nanning\",\n",
    "    \"Kiungchow\",\n",
    "    \"Pakhoi\",\n",
    "    \"Lungchow\",\n",
    "    \"Mengtsz\",\n",
    "    \"Szemao\",\n",
    "    \"Tengyueh\"\n",
    "]\n",
    "\n",
    "#list of all table types in vols\n",
    "g_all_table_types = [\n",
    "    'Revenue',\n",
    "    'Shipping',\n",
    "    'Reports To The Customs',\n",
    "    'Values',\n",
    "    'Imports',\n",
    "    'Exports',\n",
    "    'Inland Transit' ,\n",
    "    'Treasure',\n",
    "    'Passenger Traffic',\n",
    "    'Special'\n",
    "]\n",
    "\n",
    "#source path\n",
    "source_path = '/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95311c",
   "metadata": {},
   "source": [
    "#### Create local directory for combined csv and txt files\n",
    "\n",
    "`cp -r \".../annual_trade_reports/csv\" \".../annual_trade_reports/csv_and_txt\"`\n",
    "`cp -r \".../annual_trade_reports/txt\" \".../annual_trade_reports/csv_and_txt\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c3419a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_maker(year):\n",
    "    \n",
    "    folder_path = source_path + str(year) + '/csv_and_txt'\n",
    "    \n",
    "    files_list = os.listdir(folder_path)\n",
    "    \n",
    "    #dataframe with names of all the files in the folder \n",
    "    df = pd.DataFrame(files_list,columns=['file_name'])\n",
    "    \n",
    "    df = df.sort_values(by=\"file_name\",ascending=True).reset_index().drop(columns=['index'])\n",
    "    \n",
    "    #get file type (csv or txt)\n",
    "    df[\"file_type\"] = df[\"file_name\"].str[-3:]\n",
    "    \n",
    "    df[\"file_name\"] = df[\"file_name\"].str[:-4]\n",
    "    \n",
    "    df[\"title_port\"] = np.nan\n",
    "    \n",
    "    df['title_port'] = df['title_port'].astype(object)\n",
    "\n",
    "    df[\"title_table_type\"] = np.nan\n",
    "    \n",
    "    df['title_table_type'] = df['title_table_type'].astype(object)\n",
    "    \n",
    "    #list that will contain the page numbers of all content pages\n",
    "    content_pages = []\n",
    "    \n",
    "    #loops through each txt file in the folder\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        if row['file_type'] == 'txt':\n",
    "            \n",
    "            file_path = source_path + str(year) + '/txt/' + str(df.loc[index]['file_name'])+'.txt'\n",
    "            file = open(file_path, 'r')\n",
    "            Lines = file.readlines()\n",
    "            \n",
    "            #removes trailing characters from each line in the file\n",
    "            lines_list = [line.strip('\\n').strip('.').title() for line in Lines]\n",
    "            \n",
    "            #removes roman numerical characters from the beginning of all lines (this is for the port name titles)  \n",
    "            combinations_to_remove = [r\"I\\.?[-—─-]\", r\"Ii\\.?[-—─-]\",r\"Iii\\.?[-—─-]\",r\"Iv\\.?[-—─-]\",r\"V\\.?[-—─-]\",r\"Vi\\.?[-—─-]\",r\"Vii\\.?[-—─-]\",r\"Viii\\.?[-—─-]\",r\"Ix\\.?[-—─-]\"]\n",
    "            pattern = '|'.join(combinations_to_remove)\n",
    "            lines_list = [re.sub(pattern, \"\", line) for line in lines_list]\n",
    "            \n",
    "            #convert lists to sets\n",
    "            file_set = set(lines_list)\n",
    "            all_ports = set(port_names)\n",
    "            all_table_types = set(g_all_table_types)\n",
    "            \n",
    "            #check whether any port names occur in the file\n",
    "            common_elements_title_port = file_set.intersection(all_ports)\n",
    "\n",
    "            # Check if exactly one port name appears\n",
    "            if len(common_elements_title_port) == 1:\n",
    "                #if yes, this file is the cover/starting page for the port whose name appears  \n",
    "                df.at[index, 'title_port'] = list(common_elements_title_port)[0]\n",
    "\n",
    "            #check if any table type titles occur in the file \n",
    "            common_elements_title_table = file_set.intersection(all_table_types)\n",
    "            \n",
    "            #check if this file contains the word \"Contents\" once \n",
    "            if len(file_set.intersection({\"Contents\"})) == 1:\n",
    "                #if yes, this page is the contents page\n",
    "                content_pages.append(row['file_name'])\n",
    "            #else, check if exactly one table type name appears in this file \n",
    "            elif len(common_elements_title_table) == 1:\n",
    "                #if yes, this is the cover/starting page for that table type\n",
    "                df.at[index, 'title_table_type'] = list(common_elements_title_table)[0]\n",
    "            #else, check if exactly two different table type names appear in this file \n",
    "            elif len(common_elements_title_table) == 2:\n",
    "                #if yes, this is the cover/starting page for that table type\n",
    "                df.at[index, 'title_table_type'] = ','.join(common_elements_title_table)\n",
    "    \n",
    "    #forward fills the title column, ie. every page takes the value of the last valid/occuring title port name  \n",
    "    #df['title_port'] = df['title_port'].fillna(method='ffill')\n",
    "    df['title_port'] = df['title_port'].ffill()\n",
    "    \n",
    "    df.loc[0]['title_port']=df.loc[1]['title_port']\n",
    "    \n",
    "    #within the same port name, every page takes the value of the last valid/occuring table type   \n",
    "    #df[\"new_table_type\"] = df.groupby('title_port')['title_table_type'].fillna(method=\"ffill\")\n",
    "    df[\"new_table_type\"] = df.groupby('title_port')['title_table_type'].ffill().infer_objects(copy=False)\n",
    "    \n",
    "    #marks all the content pages\n",
    "    df.loc[df['file_name'].isin(content_pages), 'new_table_type'] = \"Contents\"\n",
    "    \n",
    "    df = df.drop(columns={\"title_table_type\"})\n",
    "    df = df.rename(columns={\"new_table_type\":\"title_table_type\"})\n",
    "    \n",
    "    df[\"year\"] = year\n",
    "    df[\"file_path\"] = df[\"file_name\"] +\".\"+ df[\"file_type\"]\n",
    "    \n",
    "    #files that come before the table types are categorized as \"Notes\"   \n",
    "    df = df.fillna(\"Notes\")\n",
    "    \n",
    "    df[\"title\"] = df[\"title_port\"]+ \"-\" +df[\"title_table_type\"]\n",
    "    \n",
    "    return df[[\"title_port\",\"title_table_type\",\"year\",\"file_path\",\"title\"]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c486d9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x5/8k4j_05j26vccc6wvb8y4cw00000gq/T/ipykernel_15758/141445153.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[0]['title_port']=df.loc[1]['title_port']\n"
     ]
    }
   ],
   "source": [
    "df_1927 = title_maker(1928)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23d88220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_port</th>\n",
       "      <th>title_table_type</th>\n",
       "      <th>year</th>\n",
       "      <th>file_path</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Notes</td>\n",
       "      <td>Notes</td>\n",
       "      <td>1928</td>\n",
       "      <td>.DS_S.ore</td>\n",
       "      <td>Notes-Notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aigun</td>\n",
       "      <td>Notes</td>\n",
       "      <td>1928</td>\n",
       "      <td>005825550_v2801pt1_00001.innodata.txt</td>\n",
       "      <td>Aigun-Notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aigun</td>\n",
       "      <td>Contents</td>\n",
       "      <td>1928</td>\n",
       "      <td>005825550_v2801pt1_00002.innodata.txt</td>\n",
       "      <td>Aigun-Contents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aigun</td>\n",
       "      <td>Notes</td>\n",
       "      <td>1928</td>\n",
       "      <td>005825550_v2801pt1_00003.innodata.txt</td>\n",
       "      <td>Aigun-Notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aigun</td>\n",
       "      <td>Notes</td>\n",
       "      <td>1928</td>\n",
       "      <td>005825550_v2801pt1_00004.innodata.csv</td>\n",
       "      <td>Aigun-Notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266</th>\n",
       "      <td>Tengyueh</td>\n",
       "      <td>Inland Transit</td>\n",
       "      <td>1928</td>\n",
       "      <td>005825550_v2804pt3_00128.innodata.csv</td>\n",
       "      <td>Tengyueh-Inland Transit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2267</th>\n",
       "      <td>Tengyueh</td>\n",
       "      <td>Inland Transit</td>\n",
       "      <td>1928</td>\n",
       "      <td>005825550_v2804pt3_00128.innodata.txt</td>\n",
       "      <td>Tengyueh-Inland Transit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>Tengyueh</td>\n",
       "      <td>Inland Transit</td>\n",
       "      <td>1928</td>\n",
       "      <td>005825550_v2804pt3_00129.innodata.txt</td>\n",
       "      <td>Tengyueh-Inland Transit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269</th>\n",
       "      <td>Tengyueh</td>\n",
       "      <td>Inland Transit</td>\n",
       "      <td>1928</td>\n",
       "      <td>005825550_v2804pt3_00130.innodata.csv</td>\n",
       "      <td>Tengyueh-Inland Transit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270</th>\n",
       "      <td>Tengyueh</td>\n",
       "      <td>Inland Transit</td>\n",
       "      <td>1928</td>\n",
       "      <td>005825550_v2804pt3_00130.innodata.txt</td>\n",
       "      <td>Tengyueh-Inland Transit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2271 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     title_port title_table_type  year                              file_path  \\\n",
       "0         Notes            Notes  1928                              .DS_S.ore   \n",
       "1         Aigun            Notes  1928  005825550_v2801pt1_00001.innodata.txt   \n",
       "2         Aigun         Contents  1928  005825550_v2801pt1_00002.innodata.txt   \n",
       "3         Aigun            Notes  1928  005825550_v2801pt1_00003.innodata.txt   \n",
       "4         Aigun            Notes  1928  005825550_v2801pt1_00004.innodata.csv   \n",
       "...         ...              ...   ...                                    ...   \n",
       "2266   Tengyueh   Inland Transit  1928  005825550_v2804pt3_00128.innodata.csv   \n",
       "2267   Tengyueh   Inland Transit  1928  005825550_v2804pt3_00128.innodata.txt   \n",
       "2268   Tengyueh   Inland Transit  1928  005825550_v2804pt3_00129.innodata.txt   \n",
       "2269   Tengyueh   Inland Transit  1928  005825550_v2804pt3_00130.innodata.csv   \n",
       "2270   Tengyueh   Inland Transit  1928  005825550_v2804pt3_00130.innodata.txt   \n",
       "\n",
       "                        title  \n",
       "0                 Notes-Notes  \n",
       "1                 Aigun-Notes  \n",
       "2              Aigun-Contents  \n",
       "3                 Aigun-Notes  \n",
       "4                 Aigun-Notes  \n",
       "...                       ...  \n",
       "2266  Tengyueh-Inland Transit  \n",
       "2267  Tengyueh-Inland Transit  \n",
       "2268  Tengyueh-Inland Transit  \n",
       "2269  Tengyueh-Inland Transit  \n",
       "2270  Tengyueh-Inland Transit  \n",
       "\n",
       "[2271 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1927"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852730c",
   "metadata": {},
   "source": [
    "# PART II: NLP ENTITIES CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a1465",
   "metadata": {},
   "source": [
    "### II(a) Create entities for all csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b85d0dbb-3cb9-4676-ad4b-1eb9736a94b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the language detector factory function\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector(seed=42)\n",
    "\n",
    "# Load the spaCy model and register the language detector factory if not already registered\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "if 'language_detector' not in nlp.pipe_names:\n",
    "    Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "    nlp.add_pipe('language_detector', last=True)\n",
    "\n",
    "@Language.component(\"filter_non_english_entities\")\n",
    "def filter_non_english_entities(doc):\n",
    "    filtered_ents = []\n",
    "    for ent in doc.ents:\n",
    "        if all(token.is_alpha for token in ent):\n",
    "            filtered_ents.append(ent)\n",
    "    doc.ents = filtered_ents\n",
    "    return doc\n",
    "\n",
    "# Add the custom filter component to remove non-English entities\n",
    "if 'filter_non_english_entities' not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"filter_non_english_entities\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f355461-0dc3-4261-88dd-c3bae743bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_entities_maker_csv_orig(year):\n",
    "    folder_path_csv = source_path + str(year) + '/csv'\n",
    "    files_list = os.listdir(folder_path_csv)\n",
    "    csv_files_df = pd.DataFrame(files_list,columns = ['file_name'])\n",
    "    csv_files_df['nlp_entities'] = '[]'\n",
    "    \n",
    "    for index, row in csv_files_df.iterrows():\n",
    "    \n",
    "        df = pd.read_csv(folder_path_csv + '/' +str(row[\"file_name\"]))\n",
    "        \n",
    "        #method 1 works only on \"Special\" files that have names of specific goods \n",
    "        try:\n",
    "            df[\"DESCRIPTION OF GOODS.\"] = df[\"DESCRIPTION OF GOODS.\"].fillna(\"\").astype(str)\n",
    "            \n",
    "            # Convert the description of goods column into a single string\n",
    "            combined_text = ' '.join(df[\"DESCRIPTION OF GOODS.\"].tolist())\n",
    "    \n",
    "            #NER model\n",
    "            def get_lang_detector(nlp, name):\n",
    "                return LanguageDetector(seed=42)\n",
    "    \n",
    "            nlp = spacy.load('en_core_web_sm')\n",
    "            Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "            nlp.add_pipe('language_detector', last=True)\n",
    "    \n",
    "            doc = nlp(combined_text)\n",
    "    \n",
    "            # Extract and filter named entities with the desired label ('WORK_OF_ART')\n",
    "            entities = [ent.text for ent in doc.ents if ent.label_ == 'WORK_OF_ART']\n",
    "    \n",
    "            # Count the occurrences of each named entity\n",
    "            entities_counter = Counter(entities)\n",
    "    \n",
    "            # Get the top 7 most important 'WORK_OF_ART' entities based on frequency\n",
    "            top_7_work_of_art_entities = entities_counter.most_common(7)\n",
    "    \n",
    "            csv_files_df.at[index, 'nlp_entities'] = top_7_work_of_art_entities\n",
    "        \n",
    "        #method 2: works on rest of the csv files\n",
    "        except KeyError: \n",
    "            #converts all columns to string and concatenate their values into a single string\n",
    "            combined_text = ' '.join(df.astype(str).apply(lambda x: ' '.join(x), axis=1).tolist())\n",
    "    \n",
    "            #includs the column names as potential named entities\n",
    "            column_names = list(df.columns)\n",
    "            combined_text += ' '.join(column_names)\n",
    "    \n",
    "            #NER model\n",
    "            def get_lang_detector(nlp, name):\n",
    "                return LanguageDetector(seed=42)\n",
    "                            \n",
    "            nlp = spacy.load('en_core_web_sm')\n",
    "            Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "            nlp.add_pipe('language_detector', last=True)\n",
    "    \n",
    "            doc = nlp(combined_text)\n",
    "    \n",
    "            #excludes entities with digits and entities containing 'NaN'\n",
    "            entities = [ent.text for ent in doc.ents if not any(char.isdigit() for char in ent.text) and 'nan' not in ent.text.lower()]\n",
    "    \n",
    "            #counts the occurrences of each entity\n",
    "            entity_counts = Counter(entities)\n",
    "    \n",
    "            #the 7 most common entities\n",
    "            most_common_entities = entity_counts.most_common(7)\n",
    "    \n",
    "            csv_files_df.at[index, 'nlp_entities'] = most_common_entities\n",
    "            \n",
    "            csv_files_df = csv_files_df.rename(columns={\"file_name\":\"file_path\"})\n",
    "            \n",
    "    return csv_files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07fc8fbf-1a4c-4174-877b-79a6eaa62421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_entities_maker_csv(year):\n",
    "    folder_path_csv = source_path + str(year) + '/csv'\n",
    "    files_list = os.listdir(folder_path_csv)\n",
    "    csv_files_df = pd.DataFrame(files_list, columns=['file_name'])\n",
    "    csv_files_df['nlp_entities'] = '[]'\n",
    "\n",
    "    for index, row in csv_files_df.iterrows():\n",
    "        df = pd.read_csv(folder_path_csv + '/' + str(row[\"file_name\"]))\n",
    "\n",
    "        # Load the NER model\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "        try:\n",
    "            df[\"DESCRIPTION OF GOODS.\"] = df[\"DESCRIPTION OF GOODS.\"].fillna(\"\").astype(str)\n",
    "            combined_text = ' '.join(df[\"DESCRIPTION OF GOODS.\"].tolist())\n",
    "        except KeyError:\n",
    "            combined_text = ' '.join(df.astype(str).apply(lambda x: ' '.join(x), axis=1).tolist())\n",
    "            column_names = list(df.columns)\n",
    "            combined_text += ' '.join(column_names)\n",
    "\n",
    "        # Check if 'language_detector' factory is already registered\n",
    "        if 'language_detector' not in nlp.pipe_names:\n",
    "            Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "            nlp.add_pipe('language_detector', last=True)\n",
    "\n",
    "        doc = nlp(combined_text)\n",
    "\n",
    "        entities = [ent.text for ent in doc.ents if ent.label_ == 'WORK_OF_ART' or (not any(char.isdigit() for char in ent.text) and 'nan' not in ent.text.lower())]\n",
    "        entity_counts = Counter(entities)\n",
    "        most_common_entities = entity_counts.most_common(7)\n",
    "        \n",
    "        csv_files_df.at[index, 'nlp_entities'] = most_common_entities\n",
    "        csv_files_df = csv_files_df.rename(columns={\"file_name\": \"file_path\"})\n",
    "        \n",
    "    return csv_files_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c430640",
   "metadata": {},
   "source": [
    "### II(b) Create entities for all txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72a2730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean all txt files first\n",
    "def clean_txt_files(year):\n",
    "    folder_path_txt = source_path + str(year) + '/txt'\n",
    "    files_list = os.listdir(folder_path_txt)\n",
    "    txt_files_df = pd.DataFrame(files_list,columns=['file_name'])\n",
    "    \n",
    "    for index, row in txt_files_df.iterrows():\n",
    "        file_path = folder_path_txt + '/' + str(row['file_name'])\n",
    "        with open(file_path, 'rb') as file:\n",
    "            dirty_txt = file.read()\n",
    "    \n",
    "        clean_data = dirty_txt.decode('utf-8', 'ignore')\n",
    "\n",
    "        # Write the clean data to the output file in text mode\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe5c7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_txt_files(1928)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "197933cf-d84a-4640-9457-39c456d2e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_entities_maker_txt(year):\n",
    "    \n",
    "    # Load the NER model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Check if 'language_detector' factory is already registered\n",
    "    if 'language_detector' not in nlp.pipe_names:\n",
    "        Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "        nlp.add_pipe('language_detector', last=True)\n",
    "\n",
    "    # Add the custom filter component to remove non-English entities\n",
    "    nlp.add_pipe(\"filter_non_english_entities\", last=True)\n",
    "\n",
    "    folder_path_txt = source_path + str(year) + '/txt'\n",
    "    files_list = os.listdir(folder_path_txt)\n",
    "    txt_files_df = pd.DataFrame(files_list, columns=['file_name'])\n",
    "    txt_files_df['nlp_entities'] = '[]'\n",
    "\n",
    "    for index, row in txt_files_df.iterrows():\n",
    "        file_path = folder_path_txt + '/' + str(row['file_name'])\n",
    "        \n",
    "        # Read the content of the text file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Excludes entities with digits and entities containing 'NaN'\n",
    "        entities = [ent.text for ent in doc.ents if not any(char.isdigit() for char in ent.text) and 'nan' not in ent.text.lower()]\n",
    "\n",
    "        # Counts the occurrences of each entity\n",
    "        entity_counts = Counter(entities)\n",
    "\n",
    "        # The 7 most common entities\n",
    "        most_common_entities = entity_counts.most_common(7)\n",
    "\n",
    "        txt_files_df.at[index, 'nlp_entities'] = most_common_entities\n",
    "        txt_files_df = txt_files_df.rename(columns={\"file_name\": \"file_path\"})\n",
    "    \n",
    "    return txt_files_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc9f21",
   "metadata": {},
   "source": [
    "# FINAL FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c56a2bf8-b508-4847-a878-ad080b9a54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spreadsheet_maker(year):\n",
    "    try:\n",
    "        csv_files_df = nlp_entities_maker_csv(year)\n",
    "        txt_files_df = nlp_entities_maker_txt(year)\n",
    "        nlp_entities_year = pd.concat([txt_files_df, csv_files_df], axis=0)\n",
    "        df_titles_year = title_maker(year)\n",
    "        all_cols_year = pd.merge(df_titles_year, nlp_entities_year, on=\"file_path\", how=\"left\")\n",
    "        return all_cols_year\n",
    "    except Exception as e:\n",
    "        print(f\"Error in spreadsheet_maker: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "862b4ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x5/8k4j_05j26vccc6wvb8y4cw00000gq/T/ipykernel_15758/141445153.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[0]['title_port']=df.loc[1]['title_port']\n"
     ]
    }
   ],
   "source": [
    "metadata = spreadsheet_maker(1928)\n",
    "metadata.to_csv(source_path + str(\"1928\") + \"/metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d152e4a-eadc-40c4-a7f7-51d49f45f9cb",
   "metadata": {},
   "source": [
    "## Testing & troubleshooting errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dca94d-dc7c-41f9-bde0-0c7aab6fcc7b",
   "metadata": {},
   "source": [
    "`csv` files must be properly formatted for this script to work. If the encoding is funny, or there's column of values that are not aligned correctly, the spreadsheet maker will fail. It usually works to test whether a file can be read into a dataframe. If it can't then you have to manually inspect it, dpending on the error message, and go from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cde5ce5a-9867-4ae7-bae2-e1617bbfbe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_csv = source_path + str(1928) + '/csv'\n",
    "files_list = os.listdir(folder_path_csv)\n",
    "csv_files_df = pd.DataFrame(files_list, columns=['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcd6f8b5-92d5-4cc5-a9e0-d173ea0d3e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>005825550_v2804pt3_00082.innodata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005825550_v2802pt3_00100.innodata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005825550_v2804pt3_00073_b.innodata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>005825550_v2802pt2_00059_b.innodata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005825550_v2801pt3_00119_95-97.innodata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>005825550_v2802pt2_00034_a.innodata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>005825550_v2803pt2_00144_b.innodata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>005825550_v2803pt1_00050_20-21.innodata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>005825550_v2802pt3_00102_22-23.innodata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>005825550_v2804pt2_00032_b.innodata.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>761 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file_name\n",
       "0          005825550_v2804pt3_00082.innodata.csv\n",
       "1          005825550_v2802pt3_00100.innodata.csv\n",
       "2        005825550_v2804pt3_00073_b.innodata.csv\n",
       "3        005825550_v2802pt2_00059_b.innodata.csv\n",
       "4    005825550_v2801pt3_00119_95-97.innodata.csv\n",
       "..                                           ...\n",
       "756      005825550_v2802pt2_00034_a.innodata.csv\n",
       "757      005825550_v2803pt2_00144_b.innodata.csv\n",
       "758  005825550_v2803pt1_00050_20-21.innodata.csv\n",
       "759  005825550_v2802pt3_00102_22-23.innodata.csv\n",
       "760      005825550_v2804pt2_00032_b.innodata.csv\n",
       "\n",
       "[761 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "885224b6-72a9-4ebb-8062-55bd5396e206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2804pt3_00082.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2802pt3_00100.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2804pt3_00073_b.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2802pt2_00059_b.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2801pt3_00119_95-97.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2801pt3_00057_b.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2804pt2_00093_c.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2802pt2_00086_a.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2802pt2_00060.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2802pt2_00017_21-22.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2803pt3_00130_a.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2804pt3_00099_b.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2802pt2_00098.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2802pt2_00067.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2801pt2_00071_59-90.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2804pt2_00092.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2804pt2_00038.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2804pt1_00063_a.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2804pt3_00109_b.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2803pt3_00080.innodata.csv\n",
      "/Users/katherinemika/Desktop/curation/historic_datasets/annual_trade_reports/1928/csv/005825550_v2804pt2_00012_106-108.innodata.csv\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 9 fields in line 14, saw 10\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m csv_files_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#print(index, row)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(folder_path_csv \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(folder_path_csv \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[0;32m~/anaconda3/envs/curation/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/curation/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/anaconda3/envs/curation/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/curation/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 9 fields in line 14, saw 10\n"
     ]
    }
   ],
   "source": [
    "for index, row in csv_files_df.iterrows():\n",
    "    #print(index, row)\n",
    "    print(folder_path_csv + '/' + str(row['file_name']))\n",
    "    df = pd.read_csv(folder_path_csv + '/' + str(row[\"file_name\"]))\n",
    "    #print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9732e6-6b58-470d-bdad-1d2cf838e834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
